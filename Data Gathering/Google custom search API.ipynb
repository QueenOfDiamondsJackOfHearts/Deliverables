{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook seeks to explore the google custom search API as a means of obtaining easily tagged data for ML purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we must download the google-api-python-client using pip\n",
    "from apiclient.discovery import build\n",
    "from GCS_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we must get the API key. Let us save this in a variable\n",
    "API_key=\"your API key here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here is a sample code obtained from the below URL which defines a google search and executes it\n",
    "#https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search\n",
    "#Note that this has been slightly adapted, and is mostly to demonstrate basic fucntionality result \n",
    "#Important, a Custom Search Engine (cse) ID must be defined or error is thrown, we will use for this cell a basic\n",
    "#search engine which searches the whole web.\n",
    "\n",
    "#For search engines and other google API need, we will use the google cloud console. For tutorial on building an \n",
    "#custom search engine, see: https://developers.google.com/custom-search/docs/tutorial/creatingcse\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "import pprint\n",
    "\n",
    "my_api_key = API_key\n",
    "cse_id= #create a custom search engine and add ID\n",
    "\n",
    "def google_search(search_term, api_key, cse_id, **kwargs):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()\n",
    "    return res['items'] #items returns relevant metadata about the sites returned\n",
    "\n",
    "results = google_search(\n",
    "    'stackoverflow site:en.wikipedia.org', my_api_key, cse_id , num=10)\n",
    "for result in results:\n",
    "    type(result)\n",
    "    pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "no accepted format found, ensure file links to image\n",
      "https://www.gov.gg/govgg1/images/weather/d.svg\n"
     ]
    }
   ],
   "source": [
    "#Now lets set up the basic script for lableing the data by directory\n",
    "#The plan rn is to do use a custom search query, create a directory for that query, and put all of the \n",
    "#images we get from that into that directory so that the images we retrieve are essentially labled,\n",
    "#by our query. The once this script is completed, most of the tuning will be done in the editing of the \n",
    "#search enging to give us more relevant results\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from apiclient.discovery import build\n",
    "\n",
    "API_key= #should change per user\n",
    "\n",
    "#custom search engine defined by user\n",
    "cse_id=\n",
    "\n",
    "\n",
    "#root_dir='Img_LFS' #the root directory where we will put our subdirectories(does not need to exist prior)\n",
    "root_dir='test'\n",
    "\n",
    "#precipitation queries\n",
    "#------------------\n",
    "#Query_list=['rain','snow', 'hail', 'sleet', 'precipitation', 'rainy city', 'storms', 'forecast map'] \n",
    "\n",
    "#non-precipitation queries\n",
    "#------------------\n",
    "Query_list=['sunny day -estate']#,'cloudy day -rain', 'clear night', 'clear sky', 'sunny beach', 'sunny city',\n",
    "           #'beautiful day', 'beautiful night', 'cold city -snow -rain', ] \n",
    "    \n",
    "#corresponds to the directories we will set up\n",
    "\n",
    "\n",
    "#terms used to take advantage of the advanced search features and to ensure images are mostly real\n",
    "reality= \" -cartoon -graphic -animated -cgi\" \n",
    "\n",
    "\n",
    "for q in Query_list:\n",
    "    i=1\n",
    "    #get first 100 results (limit placed by api)\n",
    "    for batch in range(1,11):\n",
    "        ind=i\n",
    "        results=img_search(q+reality, API_key, cse_id, 10, start=ind, hq=reality)\n",
    "        folder=root_dir+'/'+q\n",
    "        for result in results:\n",
    "            #name=q+'_'+str(i)+'.jpg'\n",
    "            url=result['link']\n",
    "            parsed=parse_fname(url)\n",
    "            if parsed == None:\n",
    "                name=q+'_'+str(i)+'.jpg' #if no name is parsed we try to save the data as a .jpg\n",
    "            else:\n",
    "                name=parsed\n",
    "            save_img(url,folder,name)\n",
    "            i=i+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of now, we have gathered the data set using this google custom search API. we are obviously limited in the amount of data we can collect. It may be better to use a better API, or try a web scraping approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
